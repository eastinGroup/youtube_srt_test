WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.600 align:start position:0%
 
my<00:00:00.780><c> name</c><00:00:00.930><c> is</c><00:00:01.020><c> Edgar</c><00:00:01.199><c> and</c><00:00:01.530><c> I'm</c><00:00:02.010><c> an</c><00:00:02.159><c> engineer</c><00:00:02.490><c> at</c>

00:00:02.600 --> 00:00:02.610 align:start position:0%
my name is Edgar and I'm an engineer at
 

00:00:02.610 --> 00:00:03.860 align:start position:0%
my name is Edgar and I'm an engineer at
Netflix<00:00:03.000><c> working</c><00:00:03.389><c> on</c><00:00:03.480><c> tools</c><00:00:03.689><c> and</c>

00:00:03.860 --> 00:00:03.870 align:start position:0%
Netflix working on tools and
 

00:00:03.870 --> 00:00:07.400 align:start position:0%
Netflix working on tools and
infrastructure<00:00:05.299><c> so</c><00:00:06.299><c> container</c><00:00:06.839><c> adoption</c><00:00:07.259><c> has</c>

00:00:07.400 --> 00:00:07.410 align:start position:0%
infrastructure so container adoption has
 

00:00:07.410 --> 00:00:09.440 align:start position:0%
infrastructure so container adoption has
increased<00:00:08.370><c> exponentially</c><00:00:08.849><c> over</c><00:00:09.059><c> the</c><00:00:09.240><c> years</c>

00:00:09.440 --> 00:00:09.450 align:start position:0%
increased exponentially over the years
 

00:00:09.450 --> 00:00:11.930 align:start position:0%
increased exponentially over the years
and<00:00:09.710><c> so</c><00:00:10.710><c> has</c><00:00:10.860><c> the</c><00:00:10.889><c> need</c><00:00:11.130><c> to</c><00:00:11.280><c> mass</c><00:00:11.460><c> distribute</c>

00:00:11.930 --> 00:00:11.940 align:start position:0%
and so has the need to mass distribute
 

00:00:11.940 --> 00:00:14.330 align:start position:0%
and so has the need to mass distribute
and<00:00:12.660><c> Netflix</c><00:00:13.110><c> really</c><00:00:13.440><c> launch</c><00:00:13.590><c> as</c><00:00:13.710><c> many</c><00:00:13.889><c> as</c><00:00:14.040><c> 3</c>

00:00:14.330 --> 00:00:14.340 align:start position:0%
and Netflix really launch as many as 3
 

00:00:14.340 --> 00:00:16.880 align:start position:0%
and Netflix really launch as many as 3
million<00:00:14.580><c> containers</c><00:00:14.849><c> per</c><00:00:15.150><c> week</c><00:00:15.330><c> so</c><00:00:15.900><c> the</c><00:00:16.680><c> need</c>

00:00:16.880 --> 00:00:16.890 align:start position:0%
million containers per week so the need
 

00:00:16.890 --> 00:00:19.130 align:start position:0%
million containers per week so the need
to<00:00:17.130><c> innovate</c><00:00:17.369><c> container</c><00:00:18.150><c> distribution</c><00:00:18.690><c> it's</c>

00:00:19.130 --> 00:00:19.140 align:start position:0%
to innovate container distribution it's
 

00:00:19.140 --> 00:00:22.340 align:start position:0%
to innovate container distribution it's
important<00:00:19.880><c> so</c><00:00:20.880><c> what</c><00:00:21.330><c> is</c><00:00:21.449><c> a</c><00:00:21.689><c> container</c><00:00:22.080><c> image</c>

00:00:22.340 --> 00:00:22.350 align:start position:0%
important so what is a container image
 

00:00:22.350 --> 00:00:24.800 align:start position:0%
important so what is a container image
here<00:00:23.039><c> I</c><00:00:23.070><c> have</c><00:00:23.340><c> a</c><00:00:23.400><c> 1/2</c><00:00:23.760><c> image</c><00:00:24.029><c> and</c><00:00:24.210><c> it</c><00:00:24.359><c> consists</c>

00:00:24.800 --> 00:00:24.810 align:start position:0%
here I have a 1/2 image and it consists
 

00:00:24.810 --> 00:00:27.290 align:start position:0%
here I have a 1/2 image and it consists
of<00:00:24.960><c> 4</c><00:00:25.199><c> layers</c><00:00:25.410><c> and</c><00:00:25.800><c> a</c><00:00:26.340><c> layer</c><00:00:26.550><c> it's</c><00:00:27.060><c> just</c><00:00:27.210><c> an</c>

00:00:27.290 --> 00:00:27.300 align:start position:0%
of 4 layers and a layer it's just an
 

00:00:27.300 --> 00:00:29.689 align:start position:0%
of 4 layers and a layer it's just an
archive<00:00:27.750><c> of</c><00:00:28.019><c> files</c><00:00:28.230><c> and</c><00:00:28.410><c> directories</c><00:00:28.500><c> I</c><00:00:29.070><c> can</c>

00:00:29.689 --> 00:00:29.699 align:start position:0%
archive of files and directories I can
 

00:00:29.699 --> 00:00:31.669 align:start position:0%
archive of files and directories I can
unpack<00:00:30.060><c> them</c><00:00:30.269><c> from</c><00:00:30.449><c> dr.</c><00:00:30.720><c> bottom</c><00:00:31.019><c> to</c><00:00:31.349><c> reveal</c>

00:00:31.669 --> 00:00:31.679 align:start position:0%
unpack them from dr. bottom to reveal
 

00:00:31.679 --> 00:00:32.330 align:start position:0%
unpack them from dr. bottom to reveal
its<00:00:31.830><c> contents</c>

00:00:32.330 --> 00:00:32.340 align:start position:0%
its contents
 

00:00:32.340 --> 00:00:35.450 align:start position:0%
its contents
and<00:00:32.579><c> layers</c><00:00:33.450><c> are</c><00:00:33.930><c> named</c><00:00:34.320><c> after</c><00:00:34.649><c> the</c><00:00:34.890><c> hash</c><00:00:35.100><c> of</c>

00:00:35.450 --> 00:00:35.460 align:start position:0%
and layers are named after the hash of
 

00:00:35.460 --> 00:00:39.200 align:start position:0%
and layers are named after the hash of
its<00:00:35.700><c> content</c><00:00:36.120><c> just</c><00:00:36.690><c> like</c><00:00:36.809><c> ipfs</c><00:00:38.030><c> one</c><00:00:39.030><c> key</c>

00:00:39.200 --> 00:00:39.210 align:start position:0%
its content just like ipfs one key
 

00:00:39.210 --> 00:00:41.540 align:start position:0%
its content just like ipfs one key
insight<00:00:39.570><c> here</c><00:00:39.780><c> is</c><00:00:39.809><c> that</c><00:00:40.129><c> we</c><00:00:41.129><c> can</c><00:00:41.219><c> actually</c><00:00:41.309><c> be</c>

00:00:41.540 --> 00:00:41.550 align:start position:0%
insight here is that we can actually be
 

00:00:41.550 --> 00:00:43.340 align:start position:0%
insight here is that we can actually be
couple<00:00:41.969><c> what's</c><00:00:42.210><c> the</c><00:00:42.300><c> definition</c><00:00:42.480><c> of</c><00:00:42.989><c> a</c><00:00:43.020><c> bun</c><00:00:43.170><c> -</c>

00:00:43.340 --> 00:00:43.350 align:start position:0%
couple what's the definition of a bun -
 

00:00:43.350 --> 00:00:46.250 align:start position:0%
couple what's the definition of a bun -
from<00:00:44.250><c> how</c><00:00:44.640><c> we</c><00:00:44.700><c> download</c><00:00:45.149><c> it</c><00:00:45.300><c> once</c><00:00:45.719><c> I</c><00:00:45.899><c> know</c><00:00:45.960><c> what</c>

00:00:46.250 --> 00:00:46.260 align:start position:0%
from how we download it once I know what
 

00:00:46.260 --> 00:00:48.619 align:start position:0%
from how we download it once I know what
layer<00:00:46.469><c> constituents</c><00:00:47.250><c> image</c><00:00:48.149><c> it</c><00:00:48.390><c> doesn't</c>

00:00:48.619 --> 00:00:48.629 align:start position:0%
layer constituents image it doesn't
 

00:00:48.629 --> 00:00:50.000 align:start position:0%
layer constituents image it doesn't
actually<00:00:48.719><c> matter</c><00:00:49.020><c> where</c><00:00:49.289><c> I</c><00:00:49.320><c> get</c><00:00:49.559><c> it</c><00:00:49.680><c> because</c><00:00:49.770><c> I</c>

00:00:50.000 --> 00:00:50.010 align:start position:0%
actually matter where I get it because I
 

00:00:50.010 --> 00:00:53.330 align:start position:0%
actually matter where I get it because I
can<00:00:50.160><c> independently</c><00:00:50.640><c> verify</c><00:00:51.449><c> the</c><00:00:51.510><c> hash</c><00:00:52.190><c> so</c><00:00:53.190><c> we</c>

00:00:53.330 --> 00:00:53.340 align:start position:0%
can independently verify the hash so we
 

00:00:53.340 --> 00:00:57.260 align:start position:0%
can independently verify the hash so we
can<00:00:53.460><c> leverage</c><00:00:53.730><c> ipfs</c><00:00:54.570><c> as</c><00:00:54.750><c> a</c><00:00:55.579><c> CDN</c><00:00:56.579><c> for</c><00:00:56.969><c> container</c>

00:00:57.260 --> 00:00:57.270 align:start position:0%
can leverage ipfs as a CDN for container
 

00:00:57.270 --> 00:01:00.860 align:start position:0%
can leverage ipfs as a CDN for container
layers<00:00:57.510><c> the</c><00:00:57.930><c> first</c><00:00:58.140><c> let's</c><00:00:58.379><c> visit</c><00:00:59.090><c> revisit</c><00:01:00.090><c> how</c>

00:01:00.860 --> 00:01:00.870 align:start position:0%
layers the first let's visit revisit how
 

00:01:00.870 --> 00:01:04.609 align:start position:0%
layers the first let's visit revisit how
ipfs<00:01:01.440><c> adds</c><00:01:01.710><c> files</c><00:01:02.039><c> so</c><00:01:02.879><c> a</c><00:01:03.210><c> large</c><00:01:03.719><c> file</c><00:01:03.960><c> in</c><00:01:04.110><c> ipfs</c>

00:01:04.609 --> 00:01:04.619 align:start position:0%
ipfs adds files so a large file in ipfs
 

00:01:04.619 --> 00:01:07.160 align:start position:0%
ipfs adds files so a large file in ipfs
is<00:01:04.860><c> broken</c><00:01:05.400><c> down</c><00:01:05.549><c> the</c><00:01:05.700><c> blocks</c><00:01:05.939><c> hash</c><00:01:06.540><c> to</c><00:01:06.720><c> digest</c>

00:01:07.160 --> 00:01:07.170 align:start position:0%
is broken down the blocks hash to digest
 

00:01:07.170 --> 00:01:09.170 align:start position:0%
is broken down the blocks hash to digest
and<00:01:07.350><c> then</c><00:01:07.770><c> store</c><00:01:07.950><c> it</c><00:01:07.979><c> as</c><00:01:08.189><c> a</c><00:01:08.220><c> self</c><00:01:08.850><c> described</c>

00:01:09.170 --> 00:01:09.180 align:start position:0%
and then store it as a self described
 

00:01:09.180 --> 00:01:11.480 align:start position:0%
and then store it as a self described
identifier<00:01:09.600><c> known</c><00:01:09.869><c> C</c><00:01:10.140><c> ID</c><00:01:10.260><c> and</c><00:01:10.799><c> then</c><00:01:10.920><c> hash</c><00:01:11.100><c> into</c>

00:01:11.480 --> 00:01:11.490 align:start position:0%
identifier known C ID and then hash into
 

00:01:11.490 --> 00:01:14.600 align:start position:0%
identifier known C ID and then hash into
a<00:01:11.790><c> tree</c><00:01:12.090><c> of</c><00:01:12.180><c> nodes</c><00:01:12.920><c> called</c><00:01:13.920><c> a</c><00:01:13.979><c> merkel</c><00:01:14.250><c> tag</c><00:01:14.369><c> and</c>

00:01:14.600 --> 00:01:14.610 align:start position:0%
a tree of nodes called a merkel tag and
 

00:01:14.610 --> 00:01:17.030 align:start position:0%
a tree of nodes called a merkel tag and
the<00:01:14.939><c> roots</c><00:01:15.119><c> the</c><00:01:15.270><c> ID</c><00:01:15.390><c> is</c><00:01:15.689><c> the</c><00:01:16.170><c> identifier</c><00:01:16.770><c> for</c>

00:01:17.030 --> 00:01:17.040 align:start position:0%
the roots the ID is the identifier for
 

00:01:17.040 --> 00:01:19.190 align:start position:0%
the roots the ID is the identifier for
the<00:01:17.130><c> file</c><00:01:17.310><c> coming</c><00:01:18.150><c> back</c><00:01:18.240><c> to</c><00:01:18.330><c> containers</c><00:01:18.750><c> for</c>

00:01:19.190 --> 00:01:19.200 align:start position:0%
the file coming back to containers for
 

00:01:19.200 --> 00:01:20.960 align:start position:0%
the file coming back to containers for
every<00:01:19.439><c> layer</c><00:01:19.650><c> in</c><00:01:19.799><c> a</c><00:01:19.860><c> bun</c><00:01:20.040><c> -</c><00:01:20.189><c> we</c><00:01:20.460><c> can</c><00:01:20.580><c> actually</c>

00:01:20.960 --> 00:01:20.970 align:start position:0%
every layer in a bun - we can actually
 

00:01:20.970 --> 00:01:23.149 align:start position:0%
every layer in a bun - we can actually
break<00:01:21.750><c> it</c><00:01:21.900><c> down</c><00:01:21.990><c> to</c><00:01:22.140><c> blocks</c><00:01:22.380><c> and</c><00:01:22.710><c> then</c><00:01:22.799><c> using</c>

00:01:23.149 --> 00:01:23.159 align:start position:0%
break it down to blocks and then using
 

00:01:23.159 --> 00:01:25.969 align:start position:0%
break it down to blocks and then using
the<00:01:23.310><c> roots</c><00:01:24.240><c> the</c><00:01:24.360><c> ID</c><00:01:24.560><c> translate</c><00:01:25.560><c> it</c><00:01:25.710><c> to</c><00:01:25.890><c> a</c>

00:01:25.969 --> 00:01:25.979 align:start position:0%
the roots the ID translate it to a
 

00:01:25.979 --> 00:01:27.560 align:start position:0%
the roots the ID translate it to a
digest<00:01:26.520><c> format</c><00:01:26.880><c> that</c><00:01:27.030><c> container</c><00:01:27.390><c> D</c>

00:01:27.560 --> 00:01:27.570 align:start position:0%
digest format that container D
 

00:01:27.570 --> 00:01:30.560 align:start position:0%
digest format that container D
understands<00:01:28.140><c> by</c><00:01:29.040><c> doing</c><00:01:29.340><c> this</c><00:01:29.610><c> we</c><00:01:30.030><c> created</c><00:01:30.360><c> a</c>

00:01:30.560 --> 00:01:30.570 align:start position:0%
understands by doing this we created a
 

00:01:30.570 --> 00:01:34.010 align:start position:0%
understands by doing this we created a
pure<00:01:30.810><c> pure</c><00:01:31.110><c> version</c><00:01:31.590><c> of</c><00:01:31.710><c> Ubuntu</c><00:01:32.250><c> and</c><00:01:32.900><c> then</c><00:01:33.900><c> we</c>

00:01:34.010 --> 00:01:34.020 align:start position:0%
pure pure version of Ubuntu and then we
 

00:01:34.020 --> 00:01:36.410 align:start position:0%
pure pure version of Ubuntu and then we
can<00:01:34.140><c> use</c><00:01:34.409><c> an</c><00:01:34.680><c> IP</c><00:01:34.829><c> FS</c><00:01:35.100><c> plugin</c><00:01:35.549><c> for</c><00:01:35.850><c> container</c><00:01:36.210><c> D</c>

00:01:36.410 --> 00:01:36.420 align:start position:0%
can use an IP FS plugin for container D
 

00:01:36.420 --> 00:01:38.210 align:start position:0%
can use an IP FS plugin for container D
in<00:01:36.810><c> order</c><00:01:37.049><c> to</c><00:01:37.140><c> retrieve</c><00:01:37.560><c> container</c><00:01:37.950><c> layers</c>

00:01:38.210 --> 00:01:38.220 align:start position:0%
in order to retrieve container layers
 

00:01:38.220 --> 00:01:40.819 align:start position:0%
in order to retrieve container layers
through<00:01:38.490><c> ipfs</c><00:01:39.140><c> to</c><00:01:40.140><c> demonstrate</c><00:01:40.590><c> how</c><00:01:40.680><c> this</c>

00:01:40.819 --> 00:01:40.829 align:start position:0%
through ipfs to demonstrate how this
 

00:01:40.829 --> 00:01:43.249 align:start position:0%
through ipfs to demonstrate how this
works<00:01:41.100><c> I'm</c><00:01:41.340><c> going</c><00:01:41.970><c> to</c><00:01:42.210><c> shard</c><00:01:42.659><c> a</c><00:01:42.930><c> peer-to-peer</c>

00:01:43.249 --> 00:01:43.259 align:start position:0%
works I'm going to shard a peer-to-peer
 

00:01:43.259 --> 00:01:45.950 align:start position:0%
works I'm going to shard a peer-to-peer
image<00:01:43.770><c> across</c><00:01:44.040><c> the</c><00:01:44.250><c> ec2</c><00:01:44.610><c> cluster</c><00:01:44.970><c> and</c><00:01:45.180><c> then</c>

00:01:45.950 --> 00:01:45.960 align:start position:0%
image across the ec2 cluster and then
 

00:01:45.960 --> 00:01:49.520 align:start position:0%
image across the ec2 cluster and then
try<00:01:46.890><c> to</c><00:01:46.950><c> download</c><00:01:47.130><c> it</c><00:01:47.399><c> so</c><00:01:48.259><c> sharding</c><00:01:49.259><c> we're</c>

00:01:49.520 --> 00:01:49.530 align:start position:0%
try to download it so sharding we're
 

00:01:49.530 --> 00:01:51.319 align:start position:0%
try to download it so sharding we're
gonna<00:01:49.619><c> take</c><00:01:50.100><c> every</c><00:01:50.340><c> layer</c><00:01:50.520><c> break</c><00:01:50.970><c> it</c><00:01:51.060><c> down</c><00:01:51.149><c> to</c>

00:01:51.319 --> 00:01:51.329 align:start position:0%
gonna take every layer break it down to
 

00:01:51.329 --> 00:01:53.810 align:start position:0%
gonna take every layer break it down to
chunks<00:01:51.810><c> some</c><00:01:52.470><c> of</c><00:01:52.619><c> which</c><00:01:52.740><c> may</c><00:01:52.950><c> be</c><00:01:52.979><c> duplicate</c><00:01:53.670><c> it</c>

00:01:53.810 --> 00:01:53.820 align:start position:0%
chunks some of which may be duplicate it
 

00:01:53.820 --> 00:01:57.020 align:start position:0%
chunks some of which may be duplicate it
and<00:01:53.880><c> then</c><00:01:53.970><c> group</c><00:01:54.780><c> them</c><00:01:54.990><c> into</c><00:01:55.259><c> roughly</c><00:01:55.829><c> 33%</c><00:01:56.579><c> of</c>

00:01:57.020 --> 00:01:57.030 align:start position:0%
and then group them into roughly 33% of
 

00:01:57.030 --> 00:01:59.179 align:start position:0%
and then group them into roughly 33% of
total<00:01:57.600><c> size</c><00:01:57.750><c> and</c><00:01:58.079><c> then</c><00:01:58.469><c> pin</c><00:01:58.680><c> them</c><00:01:58.890><c> on</c><00:01:59.009><c> ec2</c>

00:01:59.179 --> 00:01:59.189 align:start position:0%
total size and then pin them on ec2
 

00:01:59.189 --> 00:02:03.310 align:start position:0%
total size and then pin them on ec2
nodes<00:01:59.579><c> on</c><00:01:59.820><c> ad</c><00:02:00.240><c> bus</c>

00:02:03.310 --> 00:02:03.320 align:start position:0%
 
 

00:02:03.320 --> 00:02:06.170 align:start position:0%
 
so<00:02:04.320><c> here</c><00:02:04.680><c> I</c><00:02:04.710><c> have</c><00:02:04.829><c> a</c><00:02:04.950><c> three</c><00:02:05.070><c> easy</c><00:02:05.610><c> two</c><00:02:05.729><c> nodes</c><00:02:05.909><c> on</c>

00:02:06.170 --> 00:02:06.180 align:start position:0%
so here I have a three easy two nodes on
 

00:02:06.180 --> 00:02:10.910 align:start position:0%
so here I have a three easy two nodes on
ec2<00:02:06.930><c> and</c><00:02:07.290><c> then</c><00:02:07.829><c> I</c><00:02:08.369><c> am</c><00:02:09.030><c> going</c><00:02:09.420><c> to</c><00:02:09.750><c> check</c><00:02:10.410><c> the</c><00:02:10.590><c> IPS</c>

00:02:10.910 --> 00:02:10.920 align:start position:0%
ec2 and then I am going to check the IPS
 

00:02:10.920 --> 00:02:12.290 align:start position:0%
ec2 and then I am going to check the IPS
repo<00:02:11.280><c> and</c>

00:02:12.290 --> 00:02:12.300 align:start position:0%
repo and
 

00:02:12.300 --> 00:02:14.510 align:start position:0%
repo and
we<00:02:12.960><c> sawed-off</c><00:02:13.170><c> we're</c><00:02:13.380><c> about</c><00:02:13.530><c> 14</c><00:02:13.950><c> kilobytes</c><00:02:14.220><c> of</c>

00:02:14.510 --> 00:02:14.520 align:start position:0%
we sawed-off we're about 14 kilobytes of
 

00:02:14.520 --> 00:02:18.140 align:start position:0%
we sawed-off we're about 14 kilobytes of
data<00:02:15.380><c> what</c><00:02:16.380><c> we</c><00:02:16.470><c> can</c><00:02:16.560><c> do</c><00:02:16.650><c> now</c><00:02:16.830><c> is</c><00:02:17.160><c> a</c><00:02:17.190><c> shard</c><00:02:17.880><c> the</c>

00:02:18.140 --> 00:02:18.150 align:start position:0%
data what we can do now is a shard the
 

00:02:18.150 --> 00:02:19.460 align:start position:0%
data what we can do now is a shard the
peer-to-peer<00:02:18.360><c> image</c><00:02:18.810><c> across</c><00:02:19.080><c> the</c><00:02:19.290><c> cluster</c>

00:02:19.460 --> 00:02:19.470 align:start position:0%
peer-to-peer image across the cluster
 

00:02:19.470 --> 00:02:22.790 align:start position:0%
peer-to-peer image across the cluster
such<00:02:20.190><c> that</c><00:02:20.400><c> each</c><00:02:20.550><c> node</c><00:02:20.940><c> has</c><00:02:21.240><c> about</c><00:02:21.540><c> 33%</c><00:02:22.290><c> of</c><00:02:22.710><c> the</c>

00:02:22.790 --> 00:02:22.800 align:start position:0%
such that each node has about 33% of the
 

00:02:22.800 --> 00:02:29.330 align:start position:0%
such that each node has about 33% of the
data<00:02:24.800><c> and</c><00:02:26.600><c> then</c><00:02:27.600><c> when</c><00:02:28.350><c> we</c><00:02:28.440><c> check</c><00:02:28.650><c> IPS</c><00:02:29.040><c> again</c>

00:02:29.330 --> 00:02:29.340 align:start position:0%
data and then when we check IPS again
 

00:02:29.340 --> 00:02:31.130 align:start position:0%
data and then when we check IPS again
we'll<00:02:29.640><c> see</c><00:02:29.790><c> that</c><00:02:29.910><c> they</c><00:02:30.120><c> roughly</c><00:02:30.540><c> have</c><00:02:30.690><c> twenty</c>

00:02:31.130 --> 00:02:31.140 align:start position:0%
we'll see that they roughly have twenty
 

00:02:31.140 --> 00:02:34.760 align:start position:0%
we'll see that they roughly have twenty
ten<00:02:31.320><c> megabytes</c><00:02:31.710><c> of</c><00:02:31.860><c> data</c><00:02:33.230><c> so</c><00:02:34.230><c> what</c><00:02:34.470><c> we</c><00:02:34.560><c> want</c><00:02:34.710><c> to</c>

00:02:34.760 --> 00:02:34.770 align:start position:0%
ten megabytes of data so what we want to
 

00:02:34.770 --> 00:02:37.160 align:start position:0%
ten megabytes of data so what we want to
do<00:02:34.860><c> now</c><00:02:35.070><c> is</c><00:02:35.430><c> measure</c><00:02:36.150><c> how</c><00:02:36.420><c> long</c><00:02:36.630><c> it</c><00:02:36.780><c> takes</c><00:02:37.020><c> for</c>

00:02:37.160 --> 00:02:37.170 align:start position:0%
do now is measure how long it takes for
 

00:02:37.170 --> 00:02:39.050 align:start position:0%
do now is measure how long it takes for
every<00:02:37.680><c> node</c><00:02:37.830><c> and</c><00:02:38.010><c> network</c><00:02:38.250><c> to</c><00:02:38.400><c> receive</c><00:02:38.700><c> 100%</c>

00:02:39.050 --> 00:02:39.060 align:start position:0%
every node and network to receive 100%
 

00:02:39.060 --> 00:02:50.720 align:start position:0%
every node and network to receive 100%
of<00:02:39.360><c> the</c><00:02:39.450><c> data</c><00:02:41.600><c> let's</c><00:02:42.600><c> see</c><00:02:42.690><c> how</c><00:02:42.750><c> that</c><00:02:42.900><c> works</c><00:02:49.730><c> so</c>

00:02:50.720 --> 00:02:50.730 align:start position:0%
of the data let's see how that works so
 

00:02:50.730 --> 00:02:52.940 align:start position:0%
of the data let's see how that works so
that<00:02:50.760><c> took</c><00:02:51.240><c> about</c><00:02:51.360><c> two</c><00:02:51.600><c> seconds</c><00:02:52.110><c> and</c><00:02:52.560><c> that's</c>

00:02:52.940 --> 00:02:52.950 align:start position:0%
that took about two seconds and that's
 

00:02:52.950 --> 00:02:55.040 align:start position:0%
that took about two seconds and that's
close<00:02:53.280><c> to</c><00:02:53.310><c> docker</c><00:02:53.700><c> hub</c><00:02:53.910><c> speed</c><00:02:54.300><c> using</c><00:02:54.600><c> Akamai</c>

00:02:55.040 --> 00:02:55.050 align:start position:0%
close to docker hub speed using Akamai
 

00:02:55.050 --> 00:02:59.210 align:start position:0%
close to docker hub speed using Akamai
CDN<00:02:56.510><c> we</c><00:02:57.510><c> track</c><00:02:57.750><c> ipfs</c><00:02:58.230><c> now</c><00:02:58.470><c> they</c><00:02:58.680><c> have</c><00:02:58.770><c> about</c><00:02:58.890><c> 28</c>

00:02:59.210 --> 00:02:59.220 align:start position:0%
CDN we track ipfs now they have about 28
 

00:02:59.220 --> 00:03:01.490 align:start position:0%
CDN we track ipfs now they have about 28
mega<00:02:59.610><c> it's</c><00:02:59.700><c> data</c><00:02:59.880><c> chicken</c><00:03:00.570><c> continuty</c><00:03:01.050><c> we</c><00:03:01.380><c> can</c>

00:03:01.490 --> 00:03:01.500 align:start position:0%
mega it's data chicken continuty we can
 

00:03:01.500 --> 00:03:03.230 align:start position:0%
mega it's data chicken continuty we can
see<00:03:01.620><c> that</c><00:03:01.770><c> the</c><00:03:01.980><c> layers</c><00:03:02.190><c> are</c><00:03:02.400><c> indeed</c><00:03:02.670><c> present</c>

00:03:03.230 --> 00:03:03.240 align:start position:0%
see that the layers are indeed present
 

00:03:03.240 --> 00:03:06.260 align:start position:0%
see that the layers are indeed present
in<00:03:03.360><c> all</c><00:03:03.570><c> the</c><00:03:03.780><c> nodes</c><00:03:04.850><c> to</c><00:03:05.850><c> make</c><00:03:06.000><c> this</c><00:03:06.060><c> more</c>

00:03:06.260 --> 00:03:06.270 align:start position:0%
in all the nodes to make this more
 

00:03:06.270 --> 00:03:08.030 align:start position:0%
in all the nodes to make this more
interesting<00:03:06.300><c> how</c><00:03:07.140><c> about</c><00:03:07.350><c> we</c><00:03:07.470><c> scale</c><00:03:07.770><c> up</c><00:03:07.800><c> this</c>

00:03:08.030 --> 00:03:08.040 align:start position:0%
interesting how about we scale up this
 

00:03:08.040 --> 00:03:09.680 align:start position:0%
interesting how about we scale up this
experiment<00:03:08.640><c> so</c><00:03:08.760><c> we're</c><00:03:09.000><c> gonna</c><00:03:09.090><c> resize</c><00:03:09.510><c> the</c>

00:03:09.680 --> 00:03:09.690 align:start position:0%
experiment so we're gonna resize the
 

00:03:09.690 --> 00:03:13.460 align:start position:0%
experiment so we're gonna resize the
server<00:03:09.930><c> group</c><00:03:10.140><c> to</c><00:03:10.410><c> 50</c><00:03:10.920><c> nodes</c><00:03:11.130><c> and</c><00:03:12.410><c> this</c><00:03:13.410><c> is</c>

00:03:13.460 --> 00:03:13.470 align:start position:0%
server group to 50 nodes and this is
 

00:03:13.470 --> 00:03:15.770 align:start position:0%
server group to 50 nodes and this is
gonna<00:03:13.620><c> take</c><00:03:13.800><c> a</c><00:03:13.830><c> while</c><00:03:13.920><c> so</c><00:03:14.400><c> we're</c><00:03:15.060><c> gonna</c><00:03:15.150><c> skip</c>

00:03:15.770 --> 00:03:15.780 align:start position:0%
gonna take a while so we're gonna skip
 

00:03:15.780 --> 00:03:21.890 align:start position:0%
gonna take a while so we're gonna skip
ahead<00:03:15.900><c> -</c><00:03:16.260><c> and</c><00:03:16.290><c> that's</c><00:03:16.590><c> done</c><00:03:20.450><c> and</c><00:03:21.450><c> now</c><00:03:21.660><c> we</c><00:03:21.720><c> have</c>

00:03:21.890 --> 00:03:21.900 align:start position:0%
ahead - and that's done and now we have
 

00:03:21.900 --> 00:03:24.140 align:start position:0%
ahead - and that's done and now we have
50<00:03:22.200><c> nodes</c><00:03:22.440><c> which</c><00:03:23.250><c> means</c><00:03:23.490><c> that</c><00:03:23.670><c> when</c><00:03:23.790><c> we</c><00:03:23.880><c> short</c>

00:03:24.140 --> 00:03:24.150 align:start position:0%
50 nodes which means that when we short
 

00:03:24.150 --> 00:03:28.040 align:start position:0%
50 nodes which means that when we short
the<00:03:24.270><c> image</c><00:03:24.540><c> each</c><00:03:24.750><c> node</c><00:03:25.170><c> as</c><00:03:25.700><c> 2%</c><00:03:26.700><c> of</c><00:03:27.090><c> the</c><00:03:27.600><c> image</c>

00:03:28.040 --> 00:03:28.050 align:start position:0%
the image each node as 2% of the image
 

00:03:28.050 --> 00:03:30.830 align:start position:0%
the image each node as 2% of the image
or<00:03:28.200><c> 7</c><00:03:29.100><c> file</c><00:03:29.310><c> blocks</c><00:03:29.640><c> each</c><00:03:29.850><c> this</c><00:03:30.600><c> is</c><00:03:30.750><c> an</c>

00:03:30.830 --> 00:03:30.840 align:start position:0%
or 7 file blocks each this is an
 

00:03:30.840 --> 00:03:32.930 align:start position:0%
or 7 file blocks each this is an
unrealistic<00:03:31.170><c> scenario</c><00:03:31.500><c> but</c><00:03:32.040><c> we're</c><00:03:32.520><c> gonna</c><00:03:32.730><c> see</c>

00:03:32.930 --> 00:03:32.940 align:start position:0%
unrealistic scenario but we're gonna see
 

00:03:32.940 --> 00:03:41.920 align:start position:0%
unrealistic scenario but we're gonna see
how<00:03:32.970><c> that</c><00:03:33.209><c> works</c>

00:03:41.920 --> 00:03:41.930 align:start position:0%
 
 

00:03:41.930 --> 00:03:43.449 align:start position:0%
 
so<00:03:42.470><c> you</c><00:03:42.560><c> see</c><00:03:42.709><c> there's</c><00:03:42.920><c> a</c><00:03:42.950><c> lot</c><00:03:43.069><c> of</c><00:03:43.189><c> network</c>

00:03:43.449 --> 00:03:43.459 align:start position:0%
so you see there's a lot of network
 

00:03:43.459 --> 00:03:44.740 align:start position:0%
so you see there's a lot of network
chatter<00:03:43.670><c> and</c><00:03:43.879><c> that</c><00:03:43.909><c> affects</c><00:03:44.540><c> the</c>

00:03:44.740 --> 00:03:44.750 align:start position:0%
chatter and that affects the
 

00:03:44.750 --> 00:03:49.780 align:start position:0%
chatter and that affects the
distribution<00:03:45.439><c> of</c><00:03:45.590><c> the</c><00:03:45.709><c> image</c><00:03:48.010><c> so</c><00:03:49.010><c> it's</c><00:03:49.700><c> gonna</c>

00:03:49.780 --> 00:03:49.790 align:start position:0%
distribution of the image so it's gonna
 

00:03:49.790 --> 00:04:04.990 align:start position:0%
distribution of the image so it's gonna
take<00:03:49.939><c> about</c><00:03:50.060><c> 11</c><00:03:51.019><c> seconds</c><00:03:55.329><c> can</c><00:03:56.329><c> I</c><00:03:56.360><c> go</c><00:03:56.480><c> next</c>

00:04:04.990 --> 00:04:05.000 align:start position:0%
 
 

00:04:05.000 --> 00:04:07.660 align:start position:0%
 
so<00:04:05.510><c> container</c><00:04:06.170><c> the</c><00:04:06.650><c> container</c><00:04:06.890><c> ecosystem</c>

00:04:07.660 --> 00:04:07.670 align:start position:0%
so container the container ecosystem
 

00:04:07.670 --> 00:04:09.280 align:start position:0%
so container the container ecosystem
already<00:04:08.060><c> uses</c><00:04:08.480><c> content</c><00:04:08.810><c> addressable</c><00:04:08.960><c> storage</c>

00:04:09.280 --> 00:04:09.290 align:start position:0%
already uses content addressable storage
 

00:04:09.290 --> 00:04:11.530 align:start position:0%
already uses content addressable storage
so<00:04:09.920><c> it</c><00:04:10.010><c> makes</c><00:04:10.160><c> ipfs</c><00:04:10.730><c> a</c><00:04:10.760><c> natural</c><00:04:11.150><c> fit</c>

00:04:11.530 --> 00:04:11.540 align:start position:0%
so it makes ipfs a natural fit
 

00:04:11.540 --> 00:04:13.450 align:start position:0%
so it makes ipfs a natural fit
I<00:04:11.570><c> believe</c><00:04:12.380><c> ipfs</c><00:04:12.920><c> has</c><00:04:13.100><c> a</c><00:04:13.130><c> great</c><00:04:13.340><c> opportunity</c>

00:04:13.450 --> 00:04:13.460 align:start position:0%
I believe ipfs has a great opportunity
 

00:04:13.460 --> 00:04:16.000 align:start position:0%
I believe ipfs has a great opportunity
to<00:04:14.090><c> improve</c><00:04:14.390><c> distribution</c><00:04:14.990><c> we</c><00:04:15.590><c> just</c><00:04:15.710><c> need</c><00:04:15.920><c> to</c>

00:04:16.000 --> 00:04:16.010 align:start position:0%
to improve distribution we just need to
 

00:04:16.010 --> 00:04:17.830 align:start position:0%
to improve distribution we just need to
focus<00:04:16.220><c> on</c><00:04:16.640><c> benchmarking</c><00:04:17.329><c> and</c><00:04:17.420><c> performance</c>

00:04:17.830 --> 00:04:17.840 align:start position:0%
focus on benchmarking and performance
 

00:04:17.840 --> 00:04:20.830 align:start position:0%
focus on benchmarking and performance
tuning<00:04:18.220><c> so</c><00:04:19.220><c> what's</c><00:04:19.400><c> next</c><00:04:19.700><c> a</c><00:04:20.030><c> paper</c><00:04:20.660><c> from</c>

00:04:20.830 --> 00:04:20.840 align:start position:0%
tuning so what's next a paper from
 

00:04:20.840 --> 00:04:23.860 align:start position:0%
tuning so what's next a paper from
slacker<00:04:21.670><c> sort</c><00:04:22.670><c> of</c><00:04:22.760><c> survey</c><00:04:23.150><c> 57</c><00:04:23.780><c> different</c>

00:04:23.860 --> 00:04:23.870 align:start position:0%
slacker sort of survey 57 different
 

00:04:23.870 --> 00:04:25.930 align:start position:0%
slacker sort of survey 57 different
containerized<00:04:24.620><c> applications</c><00:04:25.220><c> and</c><00:04:25.460><c> found</c>

00:04:25.930 --> 00:04:25.940 align:start position:0%
containerized applications and found
 

00:04:25.940 --> 00:04:27.580 align:start position:0%
containerized applications and found
that<00:04:26.060><c> only</c><00:04:26.360><c> about</c><00:04:26.570><c> 6%</c><00:04:26.930><c> of</c><00:04:27.230><c> the</c><00:04:27.350><c> data</c><00:04:27.500><c> is</c>

00:04:27.580 --> 00:04:27.590 align:start position:0%
that only about 6% of the data is
 

00:04:27.590 --> 00:04:29.730 align:start position:0%
that only about 6% of the data is
actually<00:04:27.740><c> used</c><00:04:28.160><c> so</c><00:04:28.550><c> what</c><00:04:28.700><c> can</c><00:04:28.790><c> actually</c><00:04:28.940><c> do</c><00:04:29.300><c> is</c>

00:04:29.730 --> 00:04:29.740 align:start position:0%
actually used so what can actually do is
 

00:04:29.740 --> 00:04:32.650 align:start position:0%
actually used so what can actually do is
pack<00:04:30.740><c> the</c><00:04:31.010><c> container</c><00:04:31.520><c> file</c><00:04:31.730><c> system</c><00:04:32.090><c> by</c><00:04:32.180><c> a</c><00:04:32.210><c> fuse</c>

00:04:32.650 --> 00:04:32.660 align:start position:0%
pack the container file system by a fuse
 

00:04:32.660 --> 00:04:34.930 align:start position:0%
pack the container file system by a fuse
and<00:04:33.050><c> then</c><00:04:33.620><c> we</c><00:04:34.130><c> can</c><00:04:34.280><c> actually</c><00:04:34.430><c> create</c><00:04:34.790><c> a</c>

00:04:34.930 --> 00:04:34.940 align:start position:0%
and then we can actually create a
 

00:04:34.940 --> 00:04:37.390 align:start position:0%
and then we can actually create a
manifest<00:04:35.630><c> that</c><00:04:35.920><c> serializes</c><00:04:36.920><c> the</c><00:04:37.040><c> metadata</c>

00:04:37.390 --> 00:04:37.400 align:start position:0%
manifest that serializes the metadata
 

00:04:37.400 --> 00:04:39.550 align:start position:0%
manifest that serializes the metadata
for<00:04:37.700><c> files</c><00:04:37.940><c> that</c><00:04:38.240><c> way</c><00:04:38.750><c> we</c><00:04:38.870><c> can</c><00:04:39.020><c> boot</c><00:04:39.260><c> up</c><00:04:39.350><c> a</c>

00:04:39.550 --> 00:04:39.560 align:start position:0%
for files that way we can boot up a
 

00:04:39.560 --> 00:04:42.430 align:start position:0%
for files that way we can boot up a
container<00:04:39.890><c> file</c><00:04:40.130><c> system</c><00:04:40.520><c> that</c><00:04:41.320><c> it's</c><00:04:42.320><c> gonna</c>

00:04:42.430 --> 00:04:42.440 align:start position:0%
container file system that it's gonna
 

00:04:42.440 --> 00:04:44.110 align:start position:0%
container file system that it's gonna
take<00:04:42.620><c> you</c><00:04:42.800><c> know</c><00:04:42.920><c> milliseconds</c><00:04:43.730><c> because</c><00:04:43.820><c> the</c>

00:04:44.110 --> 00:04:44.120 align:start position:0%
take you know milliseconds because the
 

00:04:44.120 --> 00:04:46.180 align:start position:0%
take you know milliseconds because the
manifest<00:04:44.540><c> is</c><00:04:44.600><c> only</c><00:04:44.870><c> 200</c><00:04:45.320><c> kilobytes</c><00:04:45.620><c> bate</c><00:04:45.950><c> and</c>

00:04:46.180 --> 00:04:46.190 align:start position:0%
manifest is only 200 kilobytes bate and
 

00:04:46.190 --> 00:04:48.820 align:start position:0%
manifest is only 200 kilobytes bate and
then<00:04:46.730><c> on</c><00:04:47.000><c> demand</c><00:04:47.630><c> actually</c><00:04:48.020><c> download</c><00:04:48.710><c> the</c>

00:04:48.820 --> 00:04:48.830 align:start position:0%
then on demand actually download the
 

00:04:48.830 --> 00:04:50.500 align:start position:0%
then on demand actually download the
files<00:04:49.010><c> through</c><00:04:49.190><c> the</c><00:04:49.340><c> same</c><00:04:49.580><c> ipfs</c><00:04:50.150><c> plugin</c>

00:04:50.500 --> 00:04:50.510 align:start position:0%
files through the same ipfs plugin
 

00:04:50.510 --> 00:04:53.620 align:start position:0%
files through the same ipfs plugin
through<00:04:51.250><c> the</c><00:04:52.250><c> IP</c><00:04:52.550><c> CS</c><00:04:52.880><c> which</c><00:04:53.090><c> is</c><00:04:53.210><c> the</c><00:04:53.300><c> project</c><00:04:53.600><c> I</c>

00:04:53.620 --> 00:04:53.630 align:start position:0%
through the IP CS which is the project I
 

00:04:53.630 --> 00:04:55.860 align:start position:0%
through the IP CS which is the project I
work<00:04:53.780><c> on</c><00:04:53.900><c> thank</c><00:04:54.560><c> you</c>

00:04:55.860 --> 00:04:55.870 align:start position:0%
work on thank you
 

00:04:55.870 --> 00:05:04.079 align:start position:0%
work on thank you
[Applause]

